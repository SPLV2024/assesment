# -*- coding: utf-8 -*-
"""LVADSUSR_SASwata_Pradhan_Linear_regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YxC_QqM_syoss_rnk45Lq12XyNpl5my7
"""

# Commented out IPython magic to ensure Python compatibility.
##### Standard Libraries #####
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style("whitegrid")
sns.set_context("poster")

# %matplotlib inline

##### For Preprocessing #####
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures

##### For Building the Model #####
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import make_pipeline

##### For Validation of the Model #####
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.model_selection import cross_val_score



url='https://raw.githubusercontent.com/Deepsphere-AI/LVA-Batch5-Assessment/main/Fare%20prediction.csv'
### Load the data
df = pd.read_csv(url)
print("Size of the data:", df.shape)
df.head()

df.info()



df.drop(['key','pickup_datetime'],axis=1,inplace=True)

def categorical_variable(df):
    return list(df.select_dtypes(include = ['category', 'object']))

def numerical_variable(df):
    return list(df.select_dtypes(exclude = ['category', 'object']))

numerical_summary = df.describe()
print("Summary Statistics for Numerical Variables:")
print(numerical_summary)

categorical_vars = categorical_variable(df)
print("\nDistribution of Categorical Variables:")
for var in categorical_vars:
    print("\n---", var, "---")
    print(df[var].value_counts(normalize=True))

def automatic_EDA(df):
    # Check for numerical and categorical variables
    numerical_vars = numerical_variable(df)
    categorical_vars = categorical_variable(df)

    # EDA for numerical variables
    plt.figure(figsize=(20, 8))
    for i, var in enumerate(numerical_vars, start=1):
        plt.subplot(1, len(numerical_vars), i)
        sns.histplot(df[var], kde=True, bins=50)
        plt.title(f'Distribution of {var}')
        plt.xlabel(var)
        plt.ylabel('Frequency')
    plt.tight_layout()
    plt.show()

    # EDA for categorical variables
    plt.figure(figsize=(20, 8))
    for i, var in enumerate(categorical_vars, start=1):
        plt.subplot(1, len(categorical_vars), i)
        sns.countplot(data=df, y=var)  # Changed x to y for horizontal plot
        plt.title(f'Count of {var}')
        plt.xlabel('Count')
        plt.ylabel(var)
    plt.tight_layout()
    plt.show()

    # Correlation matrix (only for numerical variables)
    plt.figure(figsize=(12, 8))
    sns.heatmap(df[numerical_vars].corr(), annot=True, cmap='coolwarm', fmt=".2f")
    plt.title('Correlation Matrix')
    plt.show()

# Perform automatic EDA
automatic_EDA(df)

def detect_outliers(df):
    # Check for numerical variables excluding datetime columns
    numerical_vars = df.select_dtypes(include=np.number).columns.tolist()



    # Initialize a dictionary to store outlier indices for each numerical column
    outlier_indices = {}

    # Detect outliers for each numerical column
    for var in numerical_vars:
        # Calculate Q1 (25th percentile) and Q3 (75th percentile)
        Q1 = df[var].quantile(0.25)
        Q3 = df[var].quantile(0.75)

        # Calculate the interquartile range (IQR)
        IQR = Q3 - Q1

        # Define the lower and upper bounds for outlier detection
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        # Find indices of outliers
        outliers = df[(df[var] < lower_bound) | (df[var] > upper_bound)].index

        # Store the indices of outliers for this column
        if len(outliers) > 0:
            outlier_indices[var] = outliers
        else:
            print(f"No outliers found in {var}")

    return outlier_indices

# Detect outliers in the dataframe
outliers_dict = detect_outliers(df)

# Print indices of outliers for each numerical column
for var, indices in outliers_dict.items():
    print(f"Outliers in {var}:")
    print(indices)
    print()

def plot_boxplots(df):
    # Check for numerical variables excluding datetime columns
    numerical_vars = df.select_dtypes(include=np.number).columns.tolist()


    # Plot boxplots for each numerical variable
    plt.figure(figsize=(15, 8))
    for i, var in enumerate(numerical_vars, start=1):
        plt.subplot(1, len(numerical_vars), i)
        sns.boxplot(y=df[var])
        plt.title(f'Boxplot of {var}')
        plt.ylabel(var)
    plt.tight_layout()
    plt.show()

# Plot boxplots for numerical variables
plot_boxplots(df)

from sklearn.impute import SimpleImputer
import numpy as np

# Define a custom imputer function
def custom_imputer(column):
    if column.skew() > 1 or column.skew() < -1:
        return column.median()
    else:
        return column.mean()

# Apply custom imputer to each column
def impute_with_custom(df):
    for col in df.columns:
        if df[col].dtype in ['int64', 'float64']:
            df[col].fillna(custom_imputer(df[col]), inplace=True)
        else:
            df[col].fillna(df[col].mode()[0], inplace=True)
    return df

# Impute using the custom imputer function
df = impute_with_custom(df)

# Check if there are any missing values left
print(df.isnull().sum())

def handle_multicollinearity(df, threshold=0):
    # Select only numerical variables
    numerical_df = df.select_dtypes(include=np.number)

    # Calculate correlation matrix
    corr_matrix = numerical_df.corr().abs()

    # Create a mask for values above the threshold
    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))

    # Find pairs of features with correlation above threshold
    pairs = np.where(corr_matrix > threshold)

    # Initialize set to store redundant features
    redundant_features = set()

    # Remove redundant features
    for i, j in zip(*pairs):
        if i != j and i not in redundant_features and j not in redundant_features:
            redundant_features.add(numerical_df.columns[j])

    # Remove redundant features from the dataframe
    df_filtered = df.drop(columns=redundant_features)

    return df_filtered

# Handle multicollinearity in the dataframe
df_filtered = handle_multicollinearity(df)

# Calculate correlation matrix for filtered dataframe
corr_matrix_filtered = df_filtered.select_dtypes(include=np.number).corr()

# Plot the correlation matrix
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix_filtered, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix (Filtered)')
plt.show()

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

def preprocess_data(df, target_column):
    # Drop the target column
    X = df.drop(columns=[target_column])

    # Select the target column
    y = df[target_column]

    # Select numerical features for standardization
    numerical_features = X.select_dtypes(include=np.number).columns.tolist()

    # Select categorical features
    categorical_features = X.select_dtypes(exclude=np.number).columns.tolist()

    # Preprocessing pipeline for numerical features
    numerical_transformer = Pipeline(steps=[
        ('scaler', StandardScaler())
    ])

    # Preprocessing pipeline for categorical features
    categorical_transformer = Pipeline(steps=[
        ('onehot', OneHotEncoder())
    ])

    # Combine preprocessing pipelines
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numerical_transformer, numerical_features),
            ('cat', categorical_transformer, categorical_features)
        ])

    # Fit and transform the data
    X_processed = preprocessor.fit_transform(X)

    return X_processed, y

# Preprocess the data
X, y = preprocess_data(df, target_column='fare_amount')

# Now you can use X and y for regression modeling

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the linear regression model
model = LinearRegression()

# Fit the model on the training data
model.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Mean Squared Error:", mse)
print("Mean Absolute Error:", mae)
print("R-squared Score:", r2)

